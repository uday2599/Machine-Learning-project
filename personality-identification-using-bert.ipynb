{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport tensorflow as tf\nfrom transformers import BertTokenizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/mbti-type/mbti_1.csv\")\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data cleaning and preprocessing\nimport re\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\n\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('stopwords')\nlemmatizer = WordNetLemmatizer()\nfrom nltk.stem import WordNetLemmatizer\n\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\nfor i in range(0, len(df)):\n  review = re.sub('https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+',' ',df['posts'][i])\n  review = re.sub('[^a-zA-Z]', ' ', review)\n  review = review.lower()\n \n\n   \n  corpus=review","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.posts = corpus","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_input_ids = np.zeros((len(df), 1500))\nX_attn_masks = np.zeros((len(df), 1500))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-cased')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_training_data(df, ids, masks, tokenizer):\n    for i, text in tqdm(enumerate(df['posts'])):\n        tokenized_text = tokenizer.encode_plus(\n            text,\n            max_length=1500, \n            truncation=True, \n            padding='max_length', \n            add_special_tokens=True,\n            return_tensors='tf'\n        )\n        ids[i, :] = tokenized_text.input_ids\n        masks[i, :] = tokenized_text.attention_mask\n    return ids, masks","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = np.zeros((len(df), 16))\nlabels.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\ndf['type of encoding'] = enc.fit_transform(df['type'])\n\ntarget = df['type of encoding']\nfor col in df:\n    if(col=='posts'):\n      continue\n    print(sorted(df[col].unique()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(['type'], axis=1)\nlabels[np.arange(len(df)), df['type of encoding'].values] = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels[1]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating a data pipeline using tensorflow dataset utility, creates batches of data for easy loading...\ndataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\ndataset.take(1) # one sample data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SentimentDatasetMapFunction(input_ids, attn_masks, labels):\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attn_masks\n    }, labels","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.map(SentimentDatasetMapFunction)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.shuffle(10000).batch(16, drop_remainder=True) # batch size, drop any left out tensor\ndataset.take(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p = 0.8\ntrain_size = int((len(df)/16)*p)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = dataset.take(train_size)\nval_dataset = dataset.skip(train_size)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFBertModel","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TFBertModel.from_pretrained('bert-base-cased')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = tf.keras.layers.Input(shape=(1500,), name='input_ids', dtype='int32')\nattn_masks = tf.keras.layers.Input(shape=(1500,), name='attention_mask', dtype='int32')\n\nbert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)\nintermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)\noutput_layer = tf.keras.layers.Dense(16, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes\n\nsentiment_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)\nsentiment_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optim = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-6)\nloss_func = tf.keras.losses.CategoricalCrossentropy()\nacc = tf.keras.metrics.CategoricalAccuracy('accuracy')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = sentiment_model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=20\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(np.array(test_input_ids), test_labels)","metadata":{},"execution_count":null,"outputs":[]}]}